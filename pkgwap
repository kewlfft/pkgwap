#!/usr/bin/python3

"""
Author
------
Kewl <xrjy@nygb.rh.bet(rot13)>

License
-------
LGPL-3.0

Contributions
-------------
1. pkgcheck
pkgwap is a fork of pkgcheck (https://github.com/onny/pkgcheck)
Jonas Heinrich <onny@project-insanity.org>

2. parched
The PKGBUILD class used by pkgwap is a fork of parched (https://github.com/sebnow/parched)
Copyright (c) 2009 Sebastian Nowicki <sebnow@gmail.com>
Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to
deal in the Software without restriction, including without limitation the
rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
sell copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:
The above copyright notice and this permission notice shall be included in
all copies or substantial portions of the Software.

Package dependencies
--------------------
In official Extra Repository:
python-xdg python-setuptools python-gobject libnotify

To do
-----
- validate pkgbuild with namcap
- ruby md5 parallel requests http://jue.li/crux/ck4up/
- print summary at the end like x packages scanned, x outdated
- option: --ignore <packages>
- packages dict is still empty at the end :/
- parse flagged out of date in AUR
- unable to parse array of pkgbuilds
- strict and looseversion: http://stackoverflow.com/questions/1714027/version-number-comparison
- version comparison https://www.python.org/dev/peps/pep-0386/
- better handle VCS packages (https://wiki.archlinux.org/index.php/VCS_package_guidelines)
- inspiration in aur-out-of-date (https://github.com/simon04/aur-out-of-date)
- inspiration in nvchecker (https://github.com/lilydjwg/nvchecker/blob/master/nvchecker/source/github.py)
"""
import os
import re
import argparse
import hashlib
import time
# import AUR.RPC as AUR # python3-aur
import urllib.request
import urllib.error
import shlex
import fileinput
import subprocess
import json
import sys
from packaging import version
from difflib import unified_diff
import gi
gi.require_version('Notify', '0.7')
from gi.repository import Notify


__version__ = "0.2.4"
__basefile__ = os.path.basename(__file__)
HTTP_TIMEOUT = 30  # Timeout in seconds for all HTTP requests

parser = argparse.ArgumentParser(description='Scan directory for PKGBUILDs, watch for upstream updates and push.')
parser.add_argument('-u', '--update', help='update PKGBUILD if new upstream version is found', action="store_true")
parser.add_argument('-m', '--make', help='make the package: update source checksums, create .SRCINFO and build', action="store_true")
parser.add_argument('-p', '--push', help='push PKGBUILD in the AUR', action="store_true")
parser.add_argument('-f', '--force', help='force update even if no new upstream or push even if no new local version', action="store_true")
parser.add_argument('-w', '--warningonly', help='only output packages with a warning message', action="store_true")
parser.add_argument('-l', '--level', type=int, default=1, dest='level', help='recursion depth for the file crawler')
parser.add_argument('-n', '--notify', help='notify if new version', action="store_true")
parser.add_argument('-d', '--diff', help='show detailed differences when status 1 is set but versions match', action="store_true")
parser.add_argument('-v', '--version', help=f'print version of {__basefile__}', action='version', version=f'%(prog)s {__version__}')
parser.add_argument('DIR', default='.', nargs='?', help='directory or file containing PKGBUILD(s)')
args = parser.parse_args()
if args.force:
    # if args.DIR == '.':
    #     parser.error("-f requires a specific DIR to be provided")
    args.level = 0


class PKGBUILD:
    """The :class:`PKGBUILD` class provides information about a package by parsing a :manpage:`PKGBUILD(5)` file.
    To instantiate a :class:`PKGBUILD` object, pass the package's file path in the constructor::

        >>> package = PKGBUILD("PKGBUILD")

    The packages metadata can then be accessed directly::

        >>> print package
        "foo 1.0-1"
        >>> print package.description
        "Example package"
    """
    _symbol_regex = re.compile(r"\$(?P<name>{[\w\d_]+}|[\w\d]+)")

    def __init__(self, name=None, fileobj=None):
        # super(PKGBUILD, self).__init__()
        self.install = ""
        self.checksums = {
            'md5': [],
            'sha1': [],
            'sha256': [],
            'sha384': [],
            'sha512': [],
            'b2': [],
        }
        self.noextract = []
        self.sources = []
        self.makedepends = []

        # Symbol lookup table
        self._var_map = {
            'pkgbase': 'base',
            'pkgname': 'name',
            'pkgver': 'version',
            'pkgdesc': 'description',
            'pkgrel': 'release',
            'source': 'sources',
            'arch': 'architectures',
            'license': 'licenses',
        }
        self._checksum_fields = (
            'md5sums',
            'sha1sums',
            'sha256sums',
            'sha384sums',
            'sha512sums',
        )
        # Symbol table
        self._symbols = {}

        if not name:
            raise ValueError("nothing to open")
        with open(name, "r") as fileobj:
            self._parse(fileobj)
        # Extension - added useful variables
        self.path = name
        self.dir = os.path.dirname(self.path)

    def _handle_assign(self, token):
        var, equals, value = token.strip().partition('=')
        # Is it an array?
        if value[0] == '(' and value[-1] == ')':
            self._symbols[var] = self._clean_array(value)
        else:
            self._symbols[var] = self._clean(value)

    def _parse(self, fileobj):
        """Parse PKGBUILD"""
        if hasattr(fileobj, "seek"):
            fileobj.seek(0)
        parser = shlex.shlex(fileobj, posix=True)
        parser.whitespace_split = True
        in_function = False
        while True:
            token = parser.get_token()
            if token is None or token == '':
                break
            # Skip escaped newlines and functions
            if token == '\n' or in_function:
                continue
            # Special case:
            # Array elements are dispersed among tokens, we have to join
            # them first
            if token.find("=(") >= 0 and not token.rfind(")") >= 0:
                in_array = True
                elements = []
                while in_array:
                    _token = parser.get_token()
                    if _token == '\n':
                        continue
                    if _token[-1] == ')':
                        # _token = '"%s")' % _token.strip(')')
                        _token = f'"{_token[:-1]}")'
                        token = token.replace('=(', '=("', 1) + '"'
                        token = " ".join((token, " ".join(elements), _token))
                        in_array = False
                    else:
                        elements.append(f'"{_token.strip()}"')
            # Assignment
            if re.match(r"^[\w\d_]+=", token):
                self._handle_assign(token)
            # Function definitions
            elif token == '{':
                in_function = True
            elif token == '}' and in_function:
                in_function = False
        self._substitute()
        self._assign_local()
        if self.release:
            self.release = int(self.release)

    def _clean(self, value):
        """Pythonize a bash string"""
        return " ".join(shlex.split(value, posix=False))

    def _clean_array(self, value):
        """Pythonize a bash array"""
        return shlex.split(value.removeprefix('(').removesuffix(')'))

    def _replace_symbol(self, matchobj):
        """Replace a regex-matched variable with its value"""
        symbol = matchobj.group('name').removeprefix('{').removesuffix('}')
        # If the symbol isn't found fallback to an empty string, like bash
        value = self._symbols.get(symbol, '')
        # substitute the symbol with the value
        # BUG: Might result in an infinite loop, oops!
        return self._symbol_regex.sub(self._replace_symbol, value)

    def _substitute(self):
        """Substitute all bash variables within values with their values"""
        for symbol, value in self._symbols.items():
            # FIXME: This is icky
            if isinstance(value, str):
                result = self._symbol_regex.sub(self._replace_symbol, value)
            else:
                result = [self._symbol_regex.sub(self._replace_symbol, x) for x in value]
            self._symbols[symbol] = result

    def _assign_local(self):
        """Assign values from _symbols to PKGBUILD variables"""
        for var, value in self._symbols.items():
            if var in self._checksum_fields:
                key = var.removesuffix('sums')
                self.checksums[key] = value
            else:
                var = self._var_map.get(var, var)
                setattr(self, var, value)

    def _source_lookup(self, matchex):
        for symbol, valuelist in self._symbols.items():
            if symbol.startswith('source') and isinstance(valuelist, list):
                for value in valuelist:
                    if re.search(matchex, value):
                        return value


def github_api(user, repo, req):
    token = os.environ.get('GITHUB_TOKEN')
    return url_json(f"https://api.github.com/repos/{user}/{repo}/{req}", token)


def url_json(url, token=None):
    r = urllib.request.Request(url)
    if token:
        r.add_header("Authorization", "token " + token)
    try:
        js = json.load(urllib.request.urlopen(r, timeout=HTTP_TIMEOUT))
    except (urllib.error.HTTPError, urllib.error.URLError):
        js = {}
    return js


def url_regex(url, regex):
    headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'}
    try:
        req = urllib.request.Request(url, headers=headers)
        with urllib.request.urlopen(req, timeout=HTTP_TIMEOUT) as response:
            content = response.read().decode("utf-8")
    except urllib.error.URLError:
        return None
    if match := re.search(regex, content):
        return match.group(1)
    return None


def url_md5(url):
    try:
        data = urllib.request.urlopen(url, timeout=HTTP_TIMEOUT).read()
        return hashlib.md5(data).hexdigest()
    except urllib.error.URLError:
        return


def url_get_headers(url):
    """Get HTTP response headers as a string, including redirect headers"""
    headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'}
    try:
        # Create a custom opener that doesn't follow redirects automatically
        class NoRedirectHandler(urllib.request.HTTPRedirectHandler):
            def redirect_request(self, req, fp, code, msg, headers, newurl):
                # Don't follow redirects, just return None to stop
                return None
        
        opener = urllib.request.build_opener(NoRedirectHandler)
        req = urllib.request.Request(url, headers=headers)
        # Use HEAD request to avoid downloading body when we only need headers
        req.get_method = lambda: 'HEAD'
        try:
            response = opener.open(req, timeout=HTTP_TIMEOUT)
            # Combine all headers into a single string for regex matching
            header_str = '\n'.join(f"{k}: {v}" for k, v in response.headers.items())
            response.close()
            return header_str
        except urllib.error.HTTPError as e:
            # For redirects (3xx), the headers are in the exception
            header_str = '\n'.join(f"{k}: {v}" for k, v in e.headers.items())
            return header_str
    except urllib.error.URLError:
        return None


def url_regex_header(url, regex):
    """Extract version from HTTP response headers using regex"""
    if header_str := url_get_headers(url):
        if match := re.search(regex, header_str, re.IGNORECASE):
            return match.group(1)
    return None


def parse_github_url(url):
    if not url:
        return None, None
    if "github.com" in url:
        pattern = r'github\.com/([^/#]+)/([^/#]+)'
    elif "github.io" in url:
        pattern = r'([^/#]+)\.github\.io/([^/#]+)'
    else:
        return None, None
    if res := re.search(pattern, url):
        return res.group(1), res.group(2)
    return None, None


def read_pkgbuild_field(pkgbuildpath, fieldname):
    """Read a field value from PKGBUILD file"""
    try:
        with open(pkgbuildpath, 'r') as f:
            for line in f:
                if line.strip().startswith(f"{fieldname}="):
                    # Extract value, handling both quoted and unquoted
                    value = line.split('=', 1)[1].strip()
                    # Remove quotes if present
                    if (value.startswith('"') and value.endswith('"')) or \
                       (value.startswith("'") and value.endswith("'")):
                        value = value[1:-1]
                    return value
    except Exception:
        return None
    return None


def write_pkgbuild_field(pkgbuildpath, fieldname, fieldvalue):
    """Write or update a field in PKGBUILD file"""
    field_line = f'{fieldname}="{fieldvalue}"'
    # First check if field exists
    with open(pkgbuildpath, 'r') as f:
        field_exists = any(
            line.strip().startswith(f"{fieldname}=")
            for line in f
        )
    
    if field_exists:
        # Update existing field
        for line in fileinput.input([pkgbuildpath], inplace=1, backup='.bak'):
            if line.strip().startswith(f"{fieldname}="):
                print(field_line)
            else:
                print(line, end='')
    else:
        # Field doesn't exist, insert it after _watch if it exists
        with open(pkgbuildpath, 'r') as f:
            lines = f.readlines()
        insert_pos = len(lines)
        for i, line in enumerate(lines):
            if line.strip().startswith('_watch='):
                insert_pos = i + 1
                break
        lines.insert(insert_pos, f'{field_line}\n')
        with open(pkgbuildpath, 'w') as f:
            f.writelines(lines)


def is_loose(ver):
    return ver[0].isdigit()


def is_canonical(version):
    return re.match(r'^([1-9]\d*!)?(0|[1-9]\d*)(\.(0|[1-9]\d*))*((a|b|rc)(0|[1-9]\d*))?(\.post(0|[1-9]\d*))?(\.dev(0|[1-9]\d*))?$', str(version)) is not None


def isnew_version(v, w):  # returns if v>w ~ 0:no 1:new 3:error
    if v == 0:
        return 0
    if not v or v.startswith('error'):
        return 3  # FAIL
    if v.startswith('unchanged') or v == w:
        return 0
    if v.startswith('changed'):
        return 1
    try:
        vparsed = version.parse(v)
        wparsed = version.parse(w)
        return int(vparsed > wparsed)  # True:1: WARNING // False:0: OKGREEN
    except version.InvalidVersion:
        return 3


class PKGmulti:
    def __init__(self):
        # AUR: aurweb RPC interface
        self.url = "https://aur.archlinux.org/rpc/?v=5&type=info"
        self.aurverrel = {}
        self.pkglist = []

    def getaurversions(self):
        for pb in self.pkglist:
            pkg_name = pb.name[0] if isinstance(pb.name, list) else pb.name
            self.url += f"&arg[]={pkg_name}"
        
        print(f"Querying AUR for {len(self.pkglist)} package(s)...")
        rjson = url_json(self.url)
        
        # Check if we got a valid response with the expected structure
        if not rjson or 'resultcount' not in rjson or 'results' not in rjson:
            print("Warning: Failed to fetch AUR package information")
            return
        
        print(f"Found {rjson['resultcount']} package(s) in AUR")
        for result in rjson["results"]:
            self.aurverrel[result["Name"]] = result["Version"]


class pkgwap:
    def __init__(self, mypkgbuild, pkgm):
        if not mypkgbuild:
            self.pkgname = 'Package name'
            self.pkgverrel = 'Local'
            self.aurverrel = 'AUR'
            self.pkgaursame = ''
            self.upstreamver = 'Upstream'
            self.isnew = 'Status'
            return
        self.isnew = 0
        self.makepkg_done = False
        self.srcinfo_done = False
        self._aur_srcinfo_content = None  # Cache for AUR .SRCINFO content (text)
        self._aur_srcinfo_bytes = None  # Cache for AUR .SRCINFO raw bytes (for MD5)
        self._is_aur_remote = None  # Cache for AUR remote check
        self.pkgbuild = mypkgbuild
        self.pkgbuildpath = mypkgbuild.path
        self.pkgbuilddir = mypkgbuild.dir
        self.pkgver = mypkgbuild.version
        self.pkgrel = mypkgbuild.release
        self.pkgverrel = self.buildverrel()

        self.pkgname = mypkgbuild.name[0] if isinstance(mypkgbuild.name, list) else mypkgbuild.name
        try:
            self.url = mypkgbuild.url
        except AttributeError:
            self.url = ""
        try:
            self.aurverrel = pkgm.aurverrel[self.pkgname]
        except KeyError:
            self.aurverrel = ""
        self.commit = None  # Initialize full commit hash
        self.upstreamver = None  # Will be set by _watch (if target_field="pkgver") or autofind
        
        # Process _watch to write to PKGBUILD fields (pkgver is special case that also populates upstreamver)
        if watch_params := mypkgbuild._symbols.get('_watch'):
            # Filter out empty strings before validation and parsing
            watch_params = [p for p in watch_params if p] if isinstance(watch_params, list) else watch_params
            if not self._validate_watch_params(watch_params, 5, '_watch'):
                for config in self._parse_watch_configs(watch_params):
                    self._process_watch_config(config)
        
        # Fall back to autofind if no result
        if self.upstreamver is None:
            self.upstreamver = self.autofindver_upstream()
        
        # Load commit information alongside version (regardless of watch method)
        self.commit = self.autofindcommit_upstream()

    def _validate_watch_params(self, params, count, name):
        """Validate watch parameters. Returns error string or None if valid."""
        if not isinstance(params, list):
            return f"{name} requires exactly {count} parameters per configuration"
        # Filter out empty strings that may be introduced by multi-line array parsing
        filtered_params = [p for p in params if p]
        if len(filtered_params) < count:
            return f"{name} requires exactly {count} parameters per configuration"
        if len(filtered_params) % count != 0:
            return f"{name} array length must be a multiple of {count} (got {len(filtered_params)} parameters)"
        return None

    def _parse_watch_configs(self, watch_params):
        """
        Parse _watch array into list of watch configurations.
        Each watch config requires exactly 5 parameters: url, data_source, action, pattern, target_field
        
        Returns list of dicts with keys: url, data_source, action, pattern, target_field
        """
        # Filter out empty strings that may be introduced by multi-line array parsing
        filtered_params = [p for p in watch_params if p]
        return [{
            'url': filtered_params[i],
            'data_source': filtered_params[i + 1].lower(),
            'action': filtered_params[i + 2].lower(),
            'pattern': filtered_params[i + 3],
            'target_field': filtered_params[i + 4]
        } for i in range(0, len(filtered_params), 5)]

    def _process_watch_config(self, config):
        """
        Process a single watch configuration and save result to target field.
        Special case: if target_field="pkgver", also populates upstreamver for comparison.
        
        Args:
            config: dict with keys: url, data_source, action, pattern, target_field
        
        Returns:
            Extracted value (str), 0 if not found, or error message (str)
        """
        url, data_source, action, pattern, target_field = config['url'], config['data_source'], config['action'], config['pattern'], config['target_field']
        
        match action:
            case "regex":
                if not pattern:
                    return f"_watch regex action requires pattern parameter"
                extract_func = url_regex_header if data_source == "header" else url_regex
                if not (extracted_value := extract_func(url, pattern)):
                    return 0
                
                # Special handling for pkgver: populate upstreamver for comparison (don't write to file during scan)
                if target_field == "pkgver":
                    # Store extracted value in upstreamver for comparison/display
                    # Actual writing to pkgver happens in update_pkgbuild() when --update is used
                    if self.upstreamver is None:
                        self.upstreamver = extracted_value
                    return extracted_value
                
                # Check if it's a canonical version
                if is_canonical(extracted_value):
                    # Canonical version: save to target field and return it
                    write_pkgbuild_field(self.pkgbuildpath, target_field, extracted_value)
                    return extracted_value
                else:
                    # Non-canonical value: store in PKGBUILD and track changes
                    stored_value = read_pkgbuild_field(self.pkgbuildpath, target_field)
                    if stored_value == extracted_value:
                        return f"unchanged ({extracted_value[:4] if len(extracted_value) > 16 else extracted_value})"
                    write_pkgbuild_field(self.pkgbuildpath, target_field, extracted_value)
                    trunc_len = 4 if stored_value else 8
                    trunc = extracted_value[:trunc_len] if len(extracted_value) > trunc_len else extracted_value
                    status = "changed" if stored_value else "initialized"
                    return f"{status} ({trunc}...)"
            case _:
                return f"_watch action must be 'regex', got '{action}'"

    def githubversion(self, url):
        user, repo = parse_github_url(url)
        if not user or not repo:
            return

        # 1. Try GitHub API - releases/latest
        release = github_api(user, repo, "releases/latest")
        if isinstance(release, dict) and (tag := release.get("tag_name")):
            if ver := re.search(r'[a-zA-Z_-]*(\d[\d.]*\d+)', tag):
                if is_canonical(ver.group(1)):
                    return ver.group(1)

        # 2. Fallback: releases.atom feed
        if ver := url_regex(
            f"https://github.com/{user}/{repo}/releases.atom",
            r"<title>[a-zA-Z_-]*(\d[\d.]*\d+).*</title>"
        ):
            if is_canonical(ver):
                return ver

        # 3. Fallback: latest commit
        commits = github_api(user, repo, "commits")
        if isinstance(commits, list) and commits and "sha" in commits[0]:
            return f"r{len(commits)}.{commits[0]['sha'][:7]}"
        return None

    def githubcommit(self, url):
        """Returns the commit hash of the latest release tag."""
        user, repo = parse_github_url(url)
        if not user or not repo:
            return None

        release = github_api(user, repo, "releases/latest")
        if not release or not (tag := release.get("tag_name")):
            return None
        
        ref = github_api(user, repo, f"git/ref/tags/{tag}")
        if not ref or "object" not in ref:
            return None

        obj = ref["object"]
        match obj.get("type"):
            case "commit":
                return obj["sha"]
            case "tag":
                try:
                    with urllib.request.urlopen(obj["url"], timeout=HTTP_TIMEOUT) as response:
                        tag_obj = json.load(response)
                    return tag_obj.get("object", {}).get("sha", "")
                except Exception:
                    return None
        return None


    def autofindver_upstream(self):  # returns message to be displayed in column 'Upstream' (version number or other)
        # if Python package look in pypi based on pkgname
        if self.pkgname.startswith('python-'):
            for n in [self.pkgname, self.pkgname.split('-', 1)[1]]:
                try:
                    ver = url_json(f"https://pypi.python.org/pypi/{n}/json")["info"]["version"]
                except (KeyError, TypeError):
                    ver = None
                if ver and is_canonical(ver):
                    return ver
        # GitHub look with GitHub API based on user and repo parsed from url
        if ver := self.githubversion(self.url):
            return ver
        if ver := self.githubversion(self.pkgbuild._source_lookup(r"github\.(com|io)")):
            return ver
        # not Python nor GitHub: md5 of url
        return 0

    def autofindcommit_upstream(self):
        """Find the full commit hash for GitHub repositories"""
        # GitHub look with GitHub API based on user and repo parsed from url
        if self.url and 'github.com' in self.url:
            return self.githubcommit(self.url)
        
        # Check sources array for GitHub URLs
        github_url = self.pkgbuild._source_lookup(r"github\.(com|io)")
        if github_url:
            return self.githubcommit(github_url)
        
        return None

    def _fetch_aur_srcinfo_content(self):
        """Fetch the .SRCINFO content from AUR (git or web), caching the result"""
        if self._aur_srcinfo_content is not None:
            return self._aur_srcinfo_content
        
        # Try to get .SRCINFO from git remote first (more accurate, avoids web cache issues)
        git_dir = os.path.join(self.pkgbuilddir, '.git')
        if os.path.exists(git_dir):
            try:
                # Check if remote is AUR (cached check)
                if self._is_aur_remote is None:
                    process = subprocess.run(['git', 'remote', 'get-url', 'origin'], 
                                           cwd=self.pkgbuilddir, capture_output=True, text=True, timeout=5)
                    self._is_aur_remote = process.returncode == 0 and 'aur.archlinux.org' in process.stdout
                
                if self._is_aur_remote:
                    # Check if origin/master exists locally before fetching
                    process = subprocess.run(['git', 'rev-parse', '--verify', 'origin/master'], 
                                           cwd=self.pkgbuilddir, capture_output=True, timeout=5)
                    if process.returncode != 0:
                        # Only fetch if origin/master doesn't exist locally
                        subprocess.run(['git', 'fetch', 'origin'], 
                                     cwd=self.pkgbuilddir, capture_output=True, timeout=10)
                    
                    # Get .SRCINFO from remote (as text for content, encode for bytes)
                    process = subprocess.run(['git', 'show', 'origin/master:.SRCINFO'], 
                                           cwd=self.pkgbuilddir, capture_output=True, text=True, timeout=5)
                    if process.returncode == 0:
                        self._aur_srcinfo_content = process.stdout
                        self._aur_srcinfo_bytes = process.stdout.encode('utf-8')
                        return self._aur_srcinfo_content
            except (subprocess.TimeoutExpired, FileNotFoundError, Exception):
                pass
        
        # Fall back to web interface if git method didn't work
        try:
            url = f'https://aur.archlinux.org/cgit/aur.git/plain/.SRCINFO?h={self.pkgname}'
            req = urllib.request.Request(url)
            with urllib.request.urlopen(req, timeout=HTTP_TIMEOUT) as response:
                # Store raw bytes first (matching original url_md5 behavior)
                raw_bytes = response.read()
                self._aur_srcinfo_bytes = raw_bytes
                # Decode for text content
                self._aur_srcinfo_content = raw_bytes.decode('utf-8')
                return self._aur_srcinfo_content
        except Exception:
            self._aur_srcinfo_content = None
            self._aur_srcinfo_bytes = None
            return None

    def isdiff_pkgaur(self):
        # Fetch AUR .SRCINFO (cached after first fetch)
        self._fetch_aur_srcinfo_content()
        
        # Compute MD5 using cached bytes (matching original behavior)
        if self._aur_srcinfo_bytes is not None:
            hashaur = hashlib.md5(self._aur_srcinfo_bytes).hexdigest()
        else:
            # Fallback: try to get hash directly from web if content fetch failed
            hashaur = url_md5(f'https://aur.archlinux.org/cgit/aur.git/plain/.SRCINFO?h={self.pkgname}')
        
        with open(os.path.join(os.path.dirname(self.pkgbuildpath), '.SRCINFO'), "rb") as f:
            data = f.read()
        hashpkg = hashlib.md5(data).hexdigest()
        self.pkgaursame = '=' if hashaur == hashpkg else '!='
        return hashaur != hashpkg  # same:false=0, diff:true=1

    def get_aur_srcinfo(self):
        """Fetch the .SRCINFO content from AUR"""
        return self._fetch_aur_srcinfo_content()

    def get_local_srcinfo(self):
        """Get the local .SRCINFO content"""
        srcinfo_path = os.path.join(os.path.dirname(self.pkgbuildpath), '.SRCINFO')
        try:
            with open(srcinfo_path, 'r', encoding='utf-8') as f:
                return f.read()
        except Exception:
            return None

    def show_srcinfo_diff(self):
        """Show differences between local and AUR .SRCINFO files"""
        local_content = self.get_local_srcinfo()
        aur_content = self.get_aur_srcinfo()
        
        if local_content is None:
            print(f"  Error: Could not read local .SRCINFO for {self.pkgname}")
            return
        if aur_content is None:
            print(f"  Error: Could not fetch AUR .SRCINFO for {self.pkgname}")
            return
        
        local_lines = local_content.splitlines()
        aur_lines = aur_content.splitlines()
        
        # Normalize lines by stripping trailing whitespace for comparison
        local_normalized = [line.rstrip() for line in local_lines]
        aur_normalized = [line.rstrip() for line in aur_lines]
        
        # Find differences
        diff = list(unified_diff(aur_normalized, local_normalized, 
                                fromfile=f'AUR/{self.pkgname}/.SRCINFO',
                                tofile=f'Local/{self.pkgname}/.SRCINFO',
                                lineterm=''))
        
        if diff:
            print(f"\n  Differences in .SRCINFO for {self.pkgname}:")
            for line in diff[:50]:  # Limit to first 50 lines
                if line.startswith('---') or line.startswith('+++'):
                    print(f"  {line}")
                elif line.startswith('@@'):
                    print(f"  {line}")
                elif line.startswith('-'):
                    print(f"  \033[91m{line}\033[0m")  # Red for removed
                elif line.startswith('+'):
                    print(f"  \033[92m{line}\033[0m")  # Green for added
                else:
                    print(f"  {line}")
            if len(diff) > 50:
                print(f"  ... (showing first 50 of {len(diff)} diff lines)")
        else:
            print(f"  No differences found (after whitespace normalization)")

    def print_line(self):
        colordict = {'OKGREEN': 92, 'WARNING': 93, 'FAIL': 91, 'HEADER': 95, 'UNDERLINE': 4, 'OKBLUE': 94, 'ENDC': 0, 'BOLD': 1}
        state = 'HEADER' if isinstance(self.isnew, str) else ('OKGREEN' if self.isnew == 0 else 'WARNING')
        if state == 'WARNING' and args.notify:
            notif = Notify.Notification.new(self.pkgname, f'Status: {self.isnew}', 'dialog-information')
            notif.set_timeout(1000 * 3600 * 12)
            notif.show()
        print(f'\033[{colordict.get(state)}m{self.pkgname:35}{self.pkgverrel:20}{self.pkgaursame:3}{self.aurverrel:20}{self.upstreamver:19}{self.isnew:6}\033[0m')

    def buildverrel(self):
        return f"{self.pkgver}-{self.pkgrel}"

    def update_pkgbuild(self):
        u = 0
        rel = self.pkgrel  # Initialize rel in case pkgrel= line is not found
        for line in fileinput.input([self.pkgbuildpath], inplace=1, backup='.bak'):
            n = line.rstrip('\n')
            if n.startswith("pkgver="):
                n = f"pkgver={self.upstreamver}"
                u += 1
            elif n.startswith("pkgrel="):
                rel = 1 if self.upstreamver != self.pkgver else self.pkgrel + 1
                n = f"pkgrel={rel}"
                u += 1
            elif n.startswith("_commit=") and self.commit:
                # Update _commit= lines with short commit hash (first 8 characters)
                n = f"_commit={self.commit[:8]}"
                u += 1
            print(n)
        self.pkgver = self.upstreamver
        self.pkgrel = rel
        self.pkgverrel = self.buildverrel()
        print(f'\033[92mPKGBUILD {u} fields updated\033[0m')
        return u != 2  # if 2 fields updated => false = 0

    def update_pkgsum(self):
        process = subprocess.run(['updpkgsums'], cwd=self.pkgbuilddir)  # print stdout, print stderr
        # output, error = process.communicate()
        return process.returncode

    def create_srcinfo(self):
        if self.srcinfo_done:
            return
        with open(os.path.join(self.pkgbuilddir, '.SRCINFO'), 'w') as file:
            process = subprocess.run(['makepkg', '--printsrcinfo'], stdout=file, cwd=self.pkgbuilddir)  # save stdout, show stderr
        # output, error = process.communicate()
        rc = process.returncode
        self.srcinfo_done = rc == 0
        return rc

    def makepkg(self):
        if self.makepkg_done:
            return
        process = subprocess.run(['makepkg', '--force', '--clean', '--nocheck'], cwd=self.pkgbuilddir)
        # add `stdout=subprocess.PIPE` parameter to capture stdout instead of printing it to stdout
        # then `process.stdout.decode('utf-8')` contains the stdout string
        rc = process.returncode
        self.makepkg_done = rc == 0
        return rc

    def git_commit(self, msg):
        process = subprocess.run(['git', 'commit', '--all', f"--message={msg}"], cwd=self.pkgbuilddir)
        return process.returncode

    def validate_before_push(self):
        """Validate package before pushing to AUR"""
        # Check if .SRCINFO exists and is current
        srcinfo_path = os.path.join(self.pkgbuilddir, '.SRCINFO')
        if not os.path.exists(srcinfo_path):
            print(f"Error: .SRCINFO missing for {self.pkgname}")
            return False
        
        return True

    def git_push_with_validation(self):
        """Push with validation"""
        # Check for uncommitted changes and commit them if found
        process = subprocess.run(['git', 'status', '--porcelain'], cwd=self.pkgbuilddir, capture_output=True, text=True)
        if process.stdout.strip():
            print(f"Uncommitted changes detected for {self.pkgname}, committing them...")
            # Determine the type of update for better commit message
            if self.upstreamver and self.upstreamver != self.pkgver:
                commit_msg = f'{self.pkgname}: update to {self.pkgverrel}'
            else:
                commit_msg = f'{self.pkgname}: bump pkgrel to {self.pkgverrel}'
            
            if self.git_commit(commit_msg):
                print(f"Failed to commit changes for {self.pkgname}")
                return 1
        
        if not self.validate_before_push():
            return 1
        
        # Run namcap validation if available
        try:
            process = subprocess.run(['namcap', 'PKGBUILD'], cwd=self.pkgbuilddir, 
                                   capture_output=True, text=True)
            if process.returncode != 0:
                print(f"Warning: namcap found issues with {self.pkgname}:")
                print(process.stdout)
                if not args.force:
                    print("Use --force to push anyway")
                    return 1
        except FileNotFoundError:
            print(f"Warning: namcap not found, skipping PKGBUILD validation for {self.pkgname}")
        
        return self.git_push()

    def git_push(self):
        process = subprocess.run(['git', 'push'], cwd=self.pkgbuilddir)
        return process.returncode


def scanpkglist(pkgm):
    for pkgbuild in pkgm.pkglist:
        package = pkgwap(pkgbuild, pkgm)
        has_diff = package.isdiff_pkgaur()
        if has_diff:
            # 1: difference between local and AUR .SRCINFO
            package.isnew |= 1
        if isnew_version(package.upstreamver, package.pkgver):
            # 2: new upstream version compared to local
            package.isnew |= 2
        if isnew_version(package.pkgverrel, package.aurverrel):
            # 4: new local version compared to AUR
            package.isnew |= 4
        if not args.warningonly or package.isnew > 0:
            package.print_line()
            # Show diff if status 1 is set but versions match
            if args.diff and (package.isnew & 1) and package.pkgverrel == package.aurverrel:
                package.show_srcinfo_diff()
        # update
        if args.update and (((package.isnew & 2) and is_canonical(package.upstreamver)) or args.force):
            if package.update_pkgbuild():
                sys.exit(1)  # error
        # make
        if args.make:
            if package.update_pkgsum() or package.create_srcinfo() or package.makepkg():
                sys.exit(1)  # error
        # push
        if args.push and (isnew_version(package.pkgverrel, package.aurverrel) or args.force):
            print(f"Pushing {package.pkgname} version {package.pkgverrel} to AUR...")
            if package.git_push_with_validation():
                print(f"Failed to push {package.pkgname}")
                sys.exit(1)  # error
            else:
                print(f"Successfully pushed {package.pkgname} to AUR")
        if package.isnew & 1:
            # fetch aur
            pass


def walklevel(mydir, level):
    base = os.path.abspath(mydir).removesuffix(os.path.sep).count(os.path.sep)
    for root, dirs, files in os.walk(mydir):
        yield root, dirs, files
        if root.count(os.path.sep) - base >= level:
            dirs.clear()


def scandir(path, level):
    if not os.path.exists(path):
        print("error: directory does not exists")
        return
    # create list of path to PKGBUILD files
    pathlist = [
        os.path.join(root, 'PKGBUILD')
        for root, dirs, files in walklevel(path, level)
        if "PKGBUILD" in files
    ]
    pathlist.sort()
    # create list of pkgbuilds (pkgm.pkglist)
    pkgm = PKGmulti()
    for p in pathlist:
        try:
            pkgm.pkglist.append(PKGBUILD(p))
        except (ValueError, TypeError):
            pass
    # query AUR RPC from all pkgbuilds version
    pkgm.getaurversions()
    # print header after AUR query
    pkgwap(None, None).print_line()
    scanpkglist(pkgm)


# Start scanning the directory for PKGBUILDs
if args.notify:
    Notify.init(__basefile__)
scandir(args.DIR, args.level)
if args.notify:
    Notify.Notification.new(__basefile__, 'done', 'dialog-information').show()
